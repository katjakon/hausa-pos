{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word-alignment-hausa.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Word Alignment for Hausa texts\n",
        "In this notebook, word alignment on Hausa texts is performed using `fast_align` and `SimAlign`"
      ],
      "metadata": {
        "id": "W0R5S12spb5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MXHSrEJ6K9oU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "350ea28f-f06e-4c49-df4e-4eea91a0bc9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word alignment with `fast_align`\n",
        "Find out more [here](https://github.com/clab/fast_align).\n",
        "\n",
        "First, install the repository:"
      ],
      "metadata": {
        "id": "EZkgm7CfqnEf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHVwodAHHUwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b74bc77-975b-4ea3-abad-27add793db14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "git clone fast_align\n",
            "cmake\n",
            "-- The C compiler identification is GNU 7.5.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Could NOT find SparseHash (missing: SPARSEHASH_INCLUDE_DIR) \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/fast_align/build\n",
            "make\n",
            "[ 16%] Building CXX object CMakeFiles/fast_align.dir/src/fast_align.cc.o\n",
            "[ 33%] Building CXX object CMakeFiles/fast_align.dir/src/ttables.cc.o\n",
            "[ 50%] Linking CXX executable fast_align\n",
            "[ 50%] Built target fast_align\n",
            "[ 66%] Building CXX object CMakeFiles/atools.dir/src/alignment_io.cc.o\n",
            "[ 83%] Building CXX object CMakeFiles/atools.dir/src/atools.cc.o\n",
            "[100%] Linking CXX executable atools\n",
            "[100%] Built target atools\n",
            "test fast align\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'fast_align'...\n",
            "CMake Deprecation Warning at CMakeLists.txt:2 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 2.8.12 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "  CMake that the project does not need compatibility with older versions.\n",
            "\n",
            "\n",
            "Usage: /content/fast_align/build/fast_align -i file.fr-en\n",
            " Standard options ([USE] = strongly recommended):\n",
            "  -i: [REQ] Input parallel corpus\n",
            "  -v: [USE] Use Dirichlet prior on lexical translation distributions\n",
            "  -d: [USE] Favor alignment points close to the monotonic diagonoal\n",
            "  -o: [USE] Optimize how close to the diagonal alignment points should be\n",
            "  -r: Run alignment in reverse (condition on target and predict source)\n",
            "  -c: Output conditional probability table\n",
            " Advanced options:\n",
            "  -I: number of iterations in EM training (default = 5)\n",
            "  -q: p_null parameter (default = 0.08)\n",
            "  -N: No null word\n",
            "  -a: alpha parameter for optional Dirichlet prior (default = 0.01)\n",
            "  -T: starting lambda for diagonal distance parameter (default = 4)\n",
            "  -s: print alignment scores (alignment ||| score, disabled by default)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "echo \"git clone fast_align\"\n",
        "git clone https://github.com/clab/fast_align.git\n",
        "cd fast_align\n",
        "mkdir -p build\n",
        "cd build\n",
        "echo \"cmake\"\n",
        "cmake ..\n",
        "echo \"make\"\n",
        "make\n",
        "echo \"test fast align\"\n",
        "/content/fast_align/build/fast_align"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward Alignment\n",
        "Next, I iterate over all files in the directory `\"drive/MyDrive/Data/parallel/` that have the extension `txt`. The file in this directory have the assumed format `source sentence ||| target sentence`. For instance, in this case Hausa would be the target language whereas Englisch would be the source language.\n",
        "Word alignment for each file is generated and stored in the directory `\"drive/MyDrive/Data/aligned/fast_align`."
      ],
      "metadata": {
        "id": "QpYwIvIyrFt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "#!/bin/bash\n",
        "declare -a FILES=(\"drive/MyDrive/Data/parallel/Tanzil-de-ha.txt\" \"drive/MyDrive/Data/parallel/Tanzil-fr-ha.txt\")\n",
        "\n",
        "for f in \"${FILES[@]}\"\n",
        "do\n",
        "  echo \"Processing $f file...\"\n",
        "  filename=$(basename -- \"$f\")\n",
        "  name=${filename%.txt}\n",
        "  echo $name\n",
        "  forwardPath=\"/content/drive/MyDrive/Data/aligned/fast_align/$name-forward.align\"\n",
        "  reversePath=\"/content/drive/MyDrive/Data/aligned/fast_align/$name-reverse.align\"\n",
        "  symPath=\"/content/drive/MyDrive/Data/aligned/fast_align/$name-sym.align\"\n",
        "  /content/fast_align/build/fast_align -i $f -v -d -o > $forwardPath\n",
        "  /content/fast_align/build/fast_align -i $f -v -d -o -r> $reversePath\n",
        "  /content/fast_align/build/atools -i $forwardPath -j $reversePath -c grow-diag-final-and > $symPath\n",
        "done"
      ],
      "metadata": {
        "id": "2h8ypyOgK6TC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b78dab70-d749-4781-e046-0172a4702db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing drive/MyDrive/Data/parallel/Tanzil-de-ha.txt file...\n",
            "Tanzil-de-ha\n",
            "Processing drive/MyDrive/Data/parallel/Tanzil-fr-ha.txt file...\n",
            "Tanzil-fr-ha\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ARG=i\n",
            "ARG=v\n",
            "ARG=d\n",
            "ARG=o\n",
            "INITIAL PASS \n",
            "..............................................\n",
            "expected target length = source length * 1.52442\n",
            "ITERATION 1\n",
            "..............................................\n",
            "  log_e likelihood: -2.15189e+07\n",
            "  log_2 likelihood: -3.10452e+07\n",
            "     cross entropy: 29.8974\n",
            "        perplexity: 1e+09\n",
            "      posterior p0: 0.08\n",
            " posterior al-feat: -0.169145\n",
            "       size counts: 3590\n",
            "ITERATION 2\n",
            "..............................................\n",
            "  log_e likelihood: -5.40292e+06\n",
            "  log_2 likelihood: -7.79477e+06\n",
            "     cross entropy: 7.50657\n",
            "        perplexity: 181.846\n",
            "      posterior p0: 0.0622104\n",
            " posterior al-feat: -0.147922\n",
            "       size counts: 3590\n",
            "  1  model al-feat: -0.137907 (tension=4)\n",
            "  2  model al-feat: -0.141624 (tension=3.79969)\n",
            "  3  model al-feat: -0.144029 (tension=3.67371)\n",
            "  4  model al-feat: -0.145542 (tension=3.59585)\n",
            "  5  model al-feat: -0.146477 (tension=3.54825)\n",
            "  6  model al-feat: -0.147048 (tension=3.51934)\n",
            "  7  model al-feat: -0.147395 (tension=3.50185)\n",
            "  8  model al-feat: -0.147605 (tension=3.4913)\n",
            "     final tension: 3.48495\n",
            "ITERATION 3\n",
            "..............................................\n",
            "  log_e likelihood: -4.72622e+06\n",
            "  log_2 likelihood: -6.81849e+06\n",
            "     cross entropy: 6.5664\n",
            "        perplexity: 94.7725\n",
            "      posterior p0: 0.0571885\n",
            " posterior al-feat: -0.152796\n",
            "       size counts: 3590\n",
            "  1  model al-feat: -0.147731 (tension=3.48495)\n",
            "  2  model al-feat: -0.149766 (tension=3.38366)\n",
            "  3  model al-feat: -0.150999 (tension=3.32305)\n",
            "  4  model al-feat: -0.151736 (tension=3.28712)\n",
            "  5  model al-feat: -0.152172 (tension=3.26591)\n",
            "  6  model al-feat: -0.15243 (tension=3.25345)\n",
            "  7  model al-feat: -0.152581 (tension=3.24613)\n",
            "  8  model al-feat: -0.15267 (tension=3.24184)\n",
            "     final tension: 3.23932\n",
            "ITERATION 4\n",
            "..............................................\n",
            "  log_e likelihood: -4.56023e+06\n",
            "  log_2 likelihood: -6.57903e+06\n",
            "     cross entropy: 6.33578\n",
            "        perplexity: 80.7721\n",
            "      posterior p0: 0.0596956\n",
            " posterior al-feat: -0.154966\n",
            "       size counts: 3590\n",
            "  1  model al-feat: -0.152722 (tension=3.23932)\n",
            "  2  model al-feat: -0.153655 (tension=3.19444)\n",
            "  3  model al-feat: -0.154203 (tension=3.16821)\n",
            "  4  model al-feat: -0.154523 (tension=3.15295)\n",
            "  5  model al-feat: -0.154709 (tension=3.14409)\n",
            "  6  model al-feat: -0.154817 (tension=3.13894)\n",
            "  7  model al-feat: -0.15488 (tension=3.13596)\n",
            "  8  model al-feat: -0.154916 (tension=3.13424)\n",
            "     final tension: 3.13324\n",
            "ITERATION 5 (FINAL)\n",
            "..............................................\n",
            "  log_e likelihood: -4.50514e+06\n",
            "  log_2 likelihood: -6.49955e+06\n",
            "     cross entropy: 6.25924\n",
            "        perplexity: 76.5983\n",
            "      posterior p0: 0\n",
            " posterior al-feat: 0\n",
            "       size counts: 3590\n",
            "ARG=i\n",
            "ARG=v\n",
            "ARG=d\n",
            "ARG=o\n",
            "ARG=r\n",
            "INITIAL PASS \n",
            "..............................................\n",
            "expected target length = source length * 1.01524\n",
            "ITERATION 1\n",
            "..............................................\n",
            "  log_e likelihood: -1.71743e+07\n",
            "  log_2 likelihood: -2.47773e+07\n",
            "     cross entropy: 29.8974\n",
            "        perplexity: 1e+09\n",
            "      posterior p0: 0.08\n",
            " posterior al-feat: -0.170858\n",
            "       size counts: 3590\n",
            "ITERATION 2\n",
            "..............................................\n",
            "  log_e likelihood: -4.80136e+06\n",
            "  log_2 likelihood: -6.9269e+06\n",
            "     cross entropy: 8.3583\n",
            "        perplexity: 328.171\n",
            "      posterior p0: 0.0622535\n",
            " posterior al-feat: -0.148673\n",
            "       size counts: 3590\n",
            "  1  model al-feat: -0.230454 (tension=4)\n",
            "  2  model al-feat: -0.187588 (tension=5.63563)\n",
            "  3  model al-feat: -0.171753 (tension=6.41394)\n",
            "  4  model al-feat: -0.163517 (tension=6.87554)\n",
            "  5  model al-feat: -0.158628 (tension=7.17242)\n",
            "  6  model al-feat: -0.155518 (tension=7.37153)\n",
            "  7  model al-feat: -0.153454 (tension=7.50842)\n",
            "  8  model al-feat: -0.152047 (tension=7.60404)\n",
            "     final tension: 7.67152\n",
            "ITERATION 3\n",
            "..............................................\n",
            "  log_e likelihood: -4.02678e+06\n",
            "  log_2 likelihood: -5.80942e+06\n",
            "     cross entropy: 7.00989\n",
            "        perplexity: 128.881\n",
            "      posterior p0: 0.0528133\n",
            " posterior al-feat: -0.113928\n",
            "       size counts: 3590\n",
            "  1  model al-feat: -0.151071 (tension=7.67152)\n",
            "  2  model al-feat: -0.141208 (tension=8.41438)\n",
            "  3  model al-feat: -0.134903 (tension=8.95998)\n",
            "  4  model al-feat: -0.130543 (tension=9.37947)\n",
            "  5  model al-feat: -0.127369 (tension=9.71176)\n",
            "  6  model al-feat: -0.124974 (tension=9.98057)\n",
            "  7  model al-feat: -0.123117 (tension=10.2015)\n",
            "  8  model al-feat: -0.121647 (tension=10.3853)\n",
            "     final tension: 10.5396\n",
            "ITERATION 4\n",
            "..............................................\n",
            "  log_e likelihood: -3.85947e+06\n",
            "  log_2 likelihood: -5.56804e+06\n",
            "     cross entropy: 6.71864\n",
            "        perplexity: 105.321\n",
            "      posterior p0: 0.0563142\n",
            " posterior al-feat: -0.101554\n",
            "       size counts: 3590\n",
            "  1  model al-feat: -0.120463 (tension=10.5396)\n",
            "  2  model al-feat: -0.117756 (tension=10.9178)\n",
            "  3  model al-feat: -0.115651 (tension=11.2419)\n",
            "  4  model al-feat: -0.113976 (tension=11.5238)\n",
            "  5  model al-feat: -0.112621 (tension=11.7723)\n",
            "  6  model al-feat: -0.111508 (tension=11.9936)\n",
            "  7  model al-feat: -0.110583 (tension=12.1927)\n",
            "  8  model al-feat: -0.109807 (tension=12.3733)\n",
            "     final tension: 12.5384\n",
            "ITERATION 5 (FINAL)\n",
            "..............................................\n",
            "  log_e likelihood: -3.81942e+06\n",
            "  log_2 likelihood: -5.51026e+06\n",
            "     cross entropy: 6.64893\n",
            "        perplexity: 100.352\n",
            "      posterior p0: 0\n",
            " posterior al-feat: 0\n",
            "       size counts: 3590\n",
            "ARG=i\n",
            "ARG=v\n",
            "ARG=d\n",
            "ARG=o\n",
            "INITIAL PASS \n",
            "...........\n",
            "expected target length = source length * 1.46683\n",
            "ITERATION 1\n",
            "...........\n",
            "  log_e likelihood: -5.44817e+06\n",
            "  log_2 likelihood: -7.86004e+06\n",
            "     cross entropy: 29.8974\n",
            "        perplexity: 1e+09\n",
            "      posterior p0: 0.08\n",
            " posterior al-feat: -0.168987\n",
            "       size counts: 2387\n",
            "ITERATION 2\n",
            "...........\n",
            "  log_e likelihood: -1.46665e+06\n",
            "  log_2 likelihood: -2.11592e+06\n",
            "     cross entropy: 8.04837\n",
            "        perplexity: 264.728\n",
            "      posterior p0: 0.0813696\n",
            " posterior al-feat: -0.144285\n",
            "       size counts: 2387\n",
            "  1  model al-feat: -0.144836 (tension=4)\n",
            "  2  model al-feat: -0.144623 (tension=4.01101)\n",
            "  3  model al-feat: -0.144493 (tension=4.01777)\n",
            "  4  model al-feat: -0.144413 (tension=4.02193)\n",
            "  5  model al-feat: -0.144364 (tension=4.02448)\n",
            "  6  model al-feat: -0.144334 (tension=4.02605)\n",
            "  7  model al-feat: -0.144315 (tension=4.02702)\n",
            "  8  model al-feat: -0.144304 (tension=4.02762)\n",
            "     final tension: 4.02799\n",
            "ITERATION 3\n",
            "...........\n",
            "  log_e likelihood: -1.23463e+06\n",
            "  log_2 likelihood: -1.78119e+06\n",
            "     cross entropy: 6.77514\n",
            "        perplexity: 109.527\n",
            "      posterior p0: 0.0666214\n",
            " posterior al-feat: -0.142769\n",
            "       size counts: 2387\n",
            "  1  model al-feat: -0.144297 (tension=4.02799)\n",
            "  2  model al-feat: -0.143711 (tension=4.05854)\n",
            "  3  model al-feat: -0.143352 (tension=4.07738)\n",
            "  4  model al-feat: -0.14313 (tension=4.08904)\n",
            "  5  model al-feat: -0.142993 (tension=4.09626)\n",
            "  6  model al-feat: -0.142908 (tension=4.10074)\n",
            "  7  model al-feat: -0.142855 (tension=4.10351)\n",
            "  8  model al-feat: -0.142823 (tension=4.10524)\n",
            "     final tension: 4.10631\n",
            "ITERATION 4\n",
            "...........\n",
            "  log_e likelihood: -1.18322e+06\n",
            "  log_2 likelihood: -1.70703e+06\n",
            "     cross entropy: 6.49304\n",
            "        perplexity: 90.0742\n",
            "      posterior p0: 0.0673351\n",
            " posterior al-feat: -0.141081\n",
            "       size counts: 2387\n",
            "  1  model al-feat: -0.142802 (tension=4.10631)\n",
            "  2  model al-feat: -0.142152 (tension=4.14075)\n",
            "  3  model al-feat: -0.14175 (tension=4.16218)\n",
            "  4  model al-feat: -0.141499 (tension=4.17556)\n",
            "  5  model al-feat: -0.141343 (tension=4.18393)\n",
            "  6  model al-feat: -0.141245 (tension=4.18918)\n",
            "  7  model al-feat: -0.141184 (tension=4.19246)\n",
            "  8  model al-feat: -0.141145 (tension=4.19452)\n",
            "     final tension: 4.19582\n",
            "ITERATION 5 (FINAL)\n",
            "...........\n",
            "  log_e likelihood: -1.1664e+06\n",
            "  log_2 likelihood: -1.68275e+06\n",
            "     cross entropy: 6.40071\n",
            "        perplexity: 84.49\n",
            "      posterior p0: 0\n",
            " posterior al-feat: 0\n",
            "       size counts: 2387\n",
            "ARG=i\n",
            "ARG=v\n",
            "ARG=d\n",
            "ARG=o\n",
            "ARG=r\n",
            "INITIAL PASS \n",
            "...........\n",
            "expected target length = source length * 1.06217\n",
            "ITERATION 1\n",
            "...........\n",
            "  log_e likelihood: -4.55083e+06\n",
            "  log_2 likelihood: -6.56546e+06\n",
            "     cross entropy: 29.8974\n",
            "        perplexity: 1e+09\n",
            "      posterior p0: 0.08\n",
            " posterior al-feat: -0.170316\n",
            "       size counts: 2387\n",
            "ITERATION 2\n",
            "...........\n",
            "  log_e likelihood: -1.33719e+06\n",
            "  log_2 likelihood: -1.92915e+06\n",
            "     cross entropy: 8.78485\n",
            "        perplexity: 441.066\n",
            "      posterior p0: 0.0784838\n",
            " posterior al-feat: -0.144502\n",
            "       size counts: 2387\n",
            "  1  model al-feat: -0.218879 (tension=4)\n",
            "  2  model al-feat: -0.18107 (tension=5.48753)\n",
            "  3  model al-feat: -0.16623 (tension=6.21888)\n",
            "  4  model al-feat: -0.158399 (tension=6.65344)\n",
            "  5  model al-feat: -0.153737 (tension=6.93137)\n",
            "  6  model al-feat: -0.15078 (tension=7.11608)\n",
            "  7  model al-feat: -0.14883 (tension=7.24163)\n",
            "  8  model al-feat: -0.147514 (tension=7.3282)\n",
            "     final tension: 7.38845\n",
            "ITERATION 3\n",
            "...........\n",
            "  log_e likelihood: -1.08697e+06\n",
            "  log_2 likelihood: -1.56817e+06\n",
            "     cross entropy: 7.14102\n",
            "        perplexity: 141.144\n",
            "      posterior p0: 0.0621689\n",
            " posterior al-feat: -0.112618\n",
            "       size counts: 2387\n",
            "  1  model al-feat: -0.146612 (tension=7.38845)\n",
            "  2  model al-feat: -0.137131 (tension=8.06832)\n",
            "  3  model al-feat: -0.131029 (tension=8.55858)\n",
            "  4  model al-feat: -0.126806 (tension=8.92679)\n",
            "  5  model al-feat: -0.123745 (tension=9.21054)\n",
            "  6  model al-feat: -0.121455 (tension=9.43307)\n",
            "  7  model al-feat: -0.119702 (tension=9.60981)\n",
            "  8  model al-feat: -0.118338 (tension=9.75148)\n",
            "     final tension: 9.86587\n",
            "ITERATION 4\n",
            "...........\n",
            "  log_e likelihood: -1.0367e+06\n",
            "  log_2 likelihood: -1.49564e+06\n",
            "     cross entropy: 6.81074\n",
            "        perplexity: 112.263\n",
            "      posterior p0: 0.0641635\n",
            " posterior al-feat: -0.101134\n",
            "       size counts: 2387\n",
            "  1  model al-feat: -0.117261 (tension=9.86587)\n",
            "  2  model al-feat: -0.114345 (tension=10.1884)\n",
            "  3  model al-feat: -0.11208 (tension=10.4526)\n",
            "  4  model al-feat: -0.110283 (tension=10.6716)\n",
            "  5  model al-feat: -0.108834 (tension=10.8545)\n",
            "  6  model al-feat: -0.107652 (tension=11.0086)\n",
            "  7  model al-feat: -0.106676 (tension=11.1389)\n",
            "  8  model al-feat: -0.105863 (tension=11.2497)\n",
            "     final tension: 11.3443\n",
            "ITERATION 5 (FINAL)\n",
            "...........\n",
            "  log_e likelihood: -1.02378e+06\n",
            "  log_2 likelihood: -1.477e+06\n",
            "     cross entropy: 6.72585\n",
            "        perplexity: 105.848\n",
            "      posterior p0: 0\n",
            " posterior al-feat: 0\n",
            "       size counts: 2387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Alignment with `SimAlign`"
      ],
      "metadata": {
        "id": "DmFBJE9aJJq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`SimAlign` relies on embeddings. First, necessary packages are installed and imported."
      ],
      "metadata": {
        "id": "KlesjsT9K0At"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install simalign\n",
        "pip install sentencepiece"
      ],
      "metadata": {
        "id": "xRlNBNmtJODH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from simalign import SentenceAligner\n",
        "import os"
      ],
      "metadata": {
        "id": "Q3NFBKdEJdw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to specify multilingual embeddings. Here, I use multilingual BERT which was finetuned on Hausa data. The model can be found [here](https://huggingface.co/Davlan/bert-base-multilingual-cased-finetuned-hausa).\n",
        "`DATA_PATH` is the directory which contains files with the parallel sentences in the following format: `source language ||| target language`. The file names are expected to have the following format `CORPUS-SOURCE-TARGET.txt`. For example, `Tanzil-ar-ha.txt`. \n",
        "In the directory `OUT_PATH`, the alignment pairs will be stored."
      ],
      "metadata": {
        "id": "9DgyMZN9LX4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\"Davlan/bert-base-multilingual-cased-finetuned-hausa\",\n",
        "          ]\n",
        "DATA_PATH = \"PATH/TO/PARALLEL/SENTENCES\"\n",
        "OUT_PATH = \"OUT/PATH\"\n",
        "\n",
        "CORPUS = \"CORPUS NAME\""
      ],
      "metadata": {
        "id": "M0prYJz6JhPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the alignment model is loaded."
      ],
      "metadata": {
        "id": "amJghzGaMZAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making an instance of our model.\n",
        "# You can specify the embedding model and all alignment settings in the constructor.\n",
        "aligner_mbert_hausa = SentenceAligner(model=models[0],\n",
        "                            token_type=\"bpe\",\n",
        "                            matching_methods=\"mai\",\n",
        "                            device=\"cuda\")"
      ],
      "metadata": {
        "id": "FwYo5wfaKdg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I define some helper functions: `read_file` reads in files in the `|||` format described above. `write_pharao` writes the alignment files where there are pairs of indices. "
      ],
      "metadata": {
        "id": "-4hiRtOsMhWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "def read_file(file_name):\n",
        "  pairs = []\n",
        "  with open(file_name, encoding=\"utf-8\") as file:\n",
        "    for line in file:\n",
        "      line = line.strip().split(\"|||\")\n",
        "      src, trg = line\n",
        "      pairs.append((src.split(), trg.split()))\n",
        "  return pairs\n",
        "\n",
        "def write_pharao(file_name, alignment_dict):\n",
        "  lang = file_name.split(\"-\")[1]\n",
        "  types = [\"mwmf\", \"inter\", \"itermax\"]\n",
        "  for t in types:\n",
        "    out_file = \"{}-{}-ha-{}.align\".format(CORPUS, lang, t)\n",
        "    path = os.path.join(OUT_PATH, out_file)\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i in range(len(alignment_dict)):\n",
        "            alignment = alignment_dict[i][t]\n",
        "            alignment = [\"{}-{}\".format(src_idx, trg_idx) for src_idx, trg_idx in alignment]\n",
        "            f.write(\"{}\\n\".format(\" \".join(alignment)))"
      ],
      "metadata": {
        "id": "AtxGUCEJJs9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the files in `DATA_PATH` are specified for which alignment should be performed. If all files should be aligned, set this to `os.listdir(DATA_PATH)`."
      ],
      "metadata": {
        "id": "P4k8UndMM9lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File names for parallel data for each language\n",
        "file_names = ['Tanzil-de-ha.txt', 'Tanzil-fr-ha.txt']"
      ],
      "metadata": {
        "id": "_9v0d9kqJ2iC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, alignment is started."
      ],
      "metadata": {
        "id": "vOfJFvpeNR8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for file in file_names:\n",
        "    print(file)\n",
        "    path = os.path.join(DATA_PATH, file)\n",
        "    sent_pairs = read_file(path)\n",
        "    n_sents = len(sent_pairs)\n",
        "    align_dict = dict()\n",
        "    for idx, (src, trg) in enumerate(sent_pairs):\n",
        "        if idx % 100 == 0:\n",
        "          print(\"{}/{} sentences aligned.\".format(idx, n_sents))\n",
        "        alignments = aligner_mbert_hausa.get_word_aligns(src, trg)\n",
        "        align_dict[idx] = alignments\n",
        "    print(\"Writing file...\")\n",
        "    write_pharao(file, align_dict)"
      ],
      "metadata": {
        "id": "3O9pSJ4CKKFy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}